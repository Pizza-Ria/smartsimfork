{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4061d9f5-cdfc-4396-bc7d-eaecb9472558",
   "metadata": {},
   "source": [
    "# Online Training\n",
    "Simulating a complex phenomenon can be computationally intensive and expensive. In some cases\n",
    "the computational model is too expensive or too slow to be used in production, for example, when\n",
    "results are needed in real time. In other cases, a correct computational model, capable of\n",
    "predicting the behavior of a complex system, might not even exist, see e.g. [this article](https://arxiv.org/abs/2104.09355).\n",
    "*Surrogate models*, or simply *surrogates*, can compute the outcome of a given experiment\n",
    "up to a reasonable degree of approximation, while requiring little computational power or time.\n",
    "\n",
    "In this notebook, a neural network is trained to act like a surrogate model and to solve a\n",
    "well-known physical problem, i.e. computing the steady state of heat diffusion. The training\n",
    "dataset is constructed by running simualations *while* the model is being trained.\n",
    "\n",
    "## 2D Heat Diffusion and Steady State\n",
    "Throughout this notebook, the heat equation will be solved on the two-dimensional domain $[0,1]\\times[0,1]$, setting the initial temperature to 0 K, except\n",
    "for some specific spots (of radius $0.05$), where heat sources at a constant temperature of 1 K are placed. Since\n",
    "all boundaries are kept at 0 K, after enough time, the domain will reach a steady state, which is the outcome\n",
    "the surrogate model will learn to compute.\n",
    "\n",
    "The problem can be solved using a finite difference scheme. To this end, a modified version of the code\n",
    "written by John Burkardt will be used. Its original version is licensed under LGPL, and so is this example.\n",
    "The code was downloaded from [this page](https://people.sc.fsu.edu/~jburkardt/py_src/fd2d_heat_steady/fd2d_heat_steady.html),\n",
    "which explains how the problem is discretized and solved.\n",
    "\n",
    "In the modified version of the code which will be used, a random number (between 1 and 5) of heat sources is placed.\n",
    "Here below, the solver is run with different initial conditions and the results are displayed.\n",
    "To keep the solution time reasonably short, the domain is discretized with a 64x64-point grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49acfb-2585-4423-9de9-3b26bd679a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from steady_state import fd2d_heat_steady_test01\n",
    "from vishelpers import pcolor_list\n",
    "import time\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "size = 32\n",
    "\n",
    "for _ in range(3):\n",
    "    u_s = fd2d_heat_steady_test01 (size, size)\n",
    "    pcolor_list(u_s, \"Left: initial temperature. Right: steady state.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b9944-b87c-4e48-9a62-52df744fa83b",
   "metadata": {},
   "source": [
    "# The Surrogate Model\n",
    "\n",
    "The surrogate model will be a Convolutional Neural Network (CNN). The \n",
    "training set will be constructed on the fly, by running\n",
    "multiple simulations.\n",
    "\n",
    "## Model Topology\n",
    "\n",
    "The initial design for the Convolutional Neural Network (CNN) used in this notebook was inspired by the NN2 model\n",
    "defined in [this paper](https://arxiv.org/abs/2102.05527). \n",
    "Half of the Neural Network (NN) computes an encoding of the initial image: each\n",
    "convolutional layer downsamples the image by a factor 4, and doubles the\n",
    "number of channels (the input has one single channel). The second\n",
    "half of the NN reverses the operations, by upscaling the image\n",
    "by a factor 4 at each layer and halving the number of channels.\n",
    "In this variation of the model, residual blocks are used instead\n",
    "of convolutional layers, and skip connection between\n",
    "layers which are \"symmetric\" with respect to the center of the NN are added\n",
    "(the output of the 1st layer is added to the input of the $n-1$-th layer, the \n",
    "ouput of the 2nd layer is added to the input of the $n-2$-th layer, and so on).\n",
    "\n",
    "The model is defined in the file `tf_model.py`, and its depth is parametrized:\n",
    "for a $64 \\times 64$ domain, the maximum depth is $6$, but with a depth of $3$ or $4$\n",
    "the results are already noteworthy (and require less resources).\n",
    "\n",
    "## Integrating SmartRedis and SmartSim in the Simulation\n",
    "\n",
    "As in the Lattice Boltzmann tutorial, we use SmartRedis to stage tensors.\n",
    "\n",
    "For each simulation, both the initial conditions and the steady state solution are put\n",
    "on the database (DB). Since the data will be used to train a Neural Network, \n",
    "an object of the type `TrainingDataUploader` is used in the code. It \\puts the data\n",
    "on the DB in batches which will be consumed by the training process.\n",
    "The function `simulate` contained in `fd_sim.py`\n",
    "is the core of the computation. Notice that samples are augmented\n",
    "before being uploaded as batches: for each simulation,\n",
    "eight rotated and reflected version of the original samples are \n",
    "uploaded.\n",
    "\n",
    "```python\n",
    "def simulate(steps, size):\n",
    "    \"\"\"Run multiple simulations and upload results\n",
    "    \n",
    "    both as tensors and as augmented samples for training.\n",
    "\n",
    "    :param steps: Number of simulations to run\n",
    "    :type steps: int\n",
    "    :param size: lateral size of the discretized domain\n",
    "    :type size: int\n",
    "    \"\"\"\n",
    "    batch_size = 50\n",
    "    samples = np.zeros((batch_size,size,size,1)).astype(np.single)\n",
    "    targets = np.zeros_like(samples).astype(np.single)\n",
    "    client = Client(None, False)\n",
    "\n",
    "    training_data_uploader = TrainingDataUploader(cluster=False, verbose=True)\n",
    "    training_data_uploader.publish_info()\n",
    "\n",
    "    for i in tqdm(range(steps)):\n",
    "        \n",
    "        u_init, u_steady = fd2d_heat_steady_test01(samples.shape[1], samples.shape[2])\n",
    "        u_init = u_init.astype(np.single)\n",
    "        u_steady = u_steady.astype(np.single)\n",
    "        dataset = create_dataset(i, u_init, u_steady)\n",
    "        client.put_dataset(dataset)\n",
    "\n",
    "        samples[i%batch_size, :, :, 0] = u_init\n",
    "        targets[i%batch_size, :, :, 0] = u_steady\n",
    "\n",
    "        if (i+1)%batch_size == 0:\n",
    "            augmented_samples, augmented_targets = augment_batch(samples, targets)\n",
    "            training_data_uploader.put_batch(augmented_samples, augmented_targets)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Starting the Experiment\n",
    "\n",
    "SmartSim, the infrastructure library, is used here to launch the\n",
    "database, the simulation, and the NN training locally, but in separate processes.\n",
    "The example is designed to run on laptops, so the local launcher is used.\n",
    "\n",
    "First, the necessary libraries are imported, and an `Experiment` instance is created.\n",
    "An `Orchestrator` database reference is initialized and launched \n",
    "to stage data between the simulation, the NN training, and this notebook where results\n",
    "will be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f9c3e-95c9-49ad-b2d4-4fa409aeb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smartredis import Client\n",
    "from smartsim import Experiment\n",
    "# Initialize an Experiment with the local launcher\n",
    "# This will be the name of the output directory that holds\n",
    "# the output from our simulation and SmartSim\n",
    "exp = Experiment(\"surrogate_training\", launcher=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c45f55-e7a4-4141-a445-85a6158eb12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Orchestrator database reference, \n",
    "# generate its output directory, and launch it locally\n",
    "db = exp.create_database(port=6780, interface=\"lo\")\n",
    "exp.generate(db, overwrite=True)\n",
    "exp.start(db)\n",
    "print(f\"Database started at address: {db.get_address()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9ef13-ae00-4888-b81b-d059736a1c25",
   "metadata": {},
   "source": [
    "## Running an Ensemble of Simulations\n",
    "\n",
    "To run the simulation, `Experiment.create_run_settings` is used to define how the\n",
    "simulation should be executed. To obtain a larger number of samples, we run\n",
    "an ensemble of simulations, i.e. multiple replicas of the simulation,\n",
    "each one producing different samples. To set up the ensemble, the settings \n",
    "are then passed to create a reference to the ensemble through a call\n",
    "to `Experiment.create_ensemble()` which\n",
    "can be used to start, monitor, and stop the simulations from this notebook. \n",
    "Notice the call to `enable_key_prefixing()`: that will make sure\n",
    "each simulation adds a prefix to the keys it uploads, to avoid\n",
    "key clashes with other ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a1489-b4c3-4736-a628-b7af433a9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set simulation parameters we can pass as executable arguments\n",
    "# Number of simulations to run in each replica\n",
    "steps = 100\n",
    "\n",
    "# create \"run settings\" for the simulation which define how\n",
    "# the simulation will be executed when passed to Experiment.start()\n",
    "settings = exp.create_run_settings(\"/opt/conda/envs/smartsim/bin/python\",\n",
    "                                   exe_args=[\"fd_sim.py\",\n",
    "                                             f\"--steps={steps}\",\n",
    "                                             f\"--size={size}\"],\n",
    "                                   env_vars={\"OMP_NUM_THREADS\": \"2\"})\n",
    "\n",
    "# Create the ensemble reference to our simulation and\n",
    "# attach needed files to be copied, configured, or symlinked into\n",
    "# the ensemble directories at runtime. \n",
    "ensemble = exp.create_ensemble(\"fd_simulation\", run_settings=settings, replicas=2)\n",
    "ensemble.attach_generator_files(to_copy=[\"fd_sim.py\", \"steady_state.py\"])\n",
    "ensemble.enable_key_prefixing()\n",
    "exp.generate(ensemble, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008d8ae-d56f-4888-a7da-f9adcd45632d",
   "metadata": {},
   "source": [
    "## Using SmartSim to Train the Neural Network\n",
    "\n",
    "The last component of the workflow is the model training.\n",
    "For this notebook, the neural network will be trained\n",
    "using TensorFlow and Keras, but PyTorch is supported too,\n",
    "and a corresponding notebook will soon be available.\n",
    "\n",
    "In the code, an object of the type `smartsim.ml.tf.DynamicDataGenerator`\n",
    "is passed to Keras's `Model.fit()` function. The data generator\n",
    "will download batches from the database before the training starts\n",
    "and then, after each epoch, it will poll the database to\n",
    "check whether new batches are available for download.\n",
    "\n",
    "The core of the training code is in `tf_training.py`:\n",
    "\n",
    "```python\n",
    "def train_model(model, epochs):\n",
    "    training_generator = DynamicDataGenerator(cluster=False, verbose=True, batch_size=25, shuffle=True)\n",
    "    print(\"Compiling NN\")\n",
    "\n",
    "    initial_learning_rate = 0.01\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=80,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.999,\n",
    "                                        epsilon=1e-07)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        model.fit(training_generator, steps_per_epoch=None, \n",
    "                  epochs=epoch+1, initial_epoch=epoch, batch_size=training_generator.batch_size,\n",
    "                  verbose=2)\n",
    "        if (epoch+1)%10 == 0:\n",
    "            store_model(model, epoch//10)\n",
    "```\n",
    "\n",
    "Notice that after each 10 epochs, a copy of the current state of the model\n",
    "is uploaded to the database. It will be used in this notebook to\n",
    "look at the evolution of the model prediction.\n",
    "\n",
    "To set up the model, run settings are defined with the parameters\n",
    "chosen for this run, and used in `Experiment.create_model()`.\n",
    "To allow the training process to check for data produced by the\n",
    "ensemble members, they must be registered as incoming entities\n",
    "by calling `Model.register_incoming_entity()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5c68d-38f3-40a5-a0c8-a7297036022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_depth = 4\n",
    "epochs = 40\n",
    "\n",
    "ml_settings = exp.create_run_settings(\"/opt/conda/envs/smartsim/bin/python\",\n",
    "                                   exe_args=[\"tf_training.py\", \n",
    "                                             f\"--depth={nn_depth}\", \n",
    "                                             f\"--epochs={epochs}\", \n",
    "                                             f\"--size={size}\"],\n",
    "                                   env_vars={\"OMP_NUM_THREADS\": \"2\"})\n",
    "\n",
    "ml_model = exp.create_model(\"tf_training\", ml_settings)\n",
    "ml_model.attach_generator_files(to_copy=[\"tf_training.py\", \"tf_model.py\"])\n",
    "for sim in ensemble.entities:\n",
    "    ml_model.register_incoming_entity(sim)\n",
    "exp.generate(ml_model, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925266c2-d014-4047-9a06-8c4657ddcfe5",
   "metadata": {},
   "source": [
    "Once the ensemble and the model are defined, they are started by passing the references\n",
    "to `Experiment.start()`.\n",
    "The workflow is started with the `block=False` argument. This runs the ensemble\n",
    "and the training in a nonblocking manner so that the data being streamed\n",
    "from the simulation, and the training process can be\n",
    "analyzed in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b2061-5de8-4039-a431-c895e0a8940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.start(ensemble, ml_model, block=False, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc14430-a7e0-4621-a86b-0d7fbd5aa97c",
   "metadata": {},
   "source": [
    "## Progress Visualization\n",
    "\n",
    "To monitor the progress made by the training process,\n",
    "the intermediate models are run on samples drawn randomly\n",
    "from the `Orchestrator` database as soon as they are available.\n",
    "\n",
    "Notice that the inference happens on the database\n",
    "through a call to `Client.run_model()`, and only\n",
    "the result is downloaded to be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96f840-0a52-47d4-b5e4-3f2f2a3a2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect a SmartRedis client to retrieve data while the\n",
    "# simulation is producing it and storing it within the\n",
    "# orchestrator database\n",
    "\n",
    "client = Client(address=db.get_address()[0], cluster=False)\n",
    "\n",
    "num_tests = 5\n",
    "rng = default_rng()\n",
    "# Choose some random samples from the simulation\n",
    "sample_indices = rng.choice(steps//5, size=num_tests, replace=False)\n",
    "sample_indices.sort()\n",
    "\n",
    "samples = []\n",
    "for sample_idx in sample_indices:\n",
    "    u_steady_name = ensemble.entities[0].name + \".{sim_data_\" + str(sample_idx)+ \"}.u_steady\"\n",
    "    client.poll_key(u_steady_name, 300, 1000)\n",
    "    samples.append(client.get_tensor(u_steady_name).squeeze())\n",
    "    \n",
    "pcolor_list(samples, \"Simulation\")\n",
    "\n",
    "for i in range(0, epochs//10):\n",
    "    nn_name = f\"DiffusionResNet_{nn_depth}_{i}\"\n",
    "    client.poll_key(nn_name, 300, 1000)\n",
    "    ml_steady = []\n",
    "    for sample_idx in sample_indices:\n",
    "        u_init_name = ensemble.entities[0].name + \".{sim_data_\" + str(sample_idx)+ \"}.u_init\"\n",
    "        client.run_model(nn_name, inputs=[u_init_name], outputs=[f\"ml_steady_{sample_idx}\"])\n",
    "        ml_steady.append(client.get_tensor(f\"ml_steady_{sample_idx}\").squeeze())\n",
    "\n",
    "    pcolor_list(ml_steady, f\"Model at training epoch {(i+1)*10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbce88c-6f63-407a-8912-5787139f015b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optionally clear the database\n",
    "client.flush_db(db.get_address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f2669-4efb-4f38-97e9-869a070ab79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Experiment API to wait until the model\n",
    "# is finished and then terminate the database and\n",
    "# release it's resources\n",
    "while not all([exp.finished(ensemble), exp.finished(ml_model)]):\n",
    "    time.sleep(5)\n",
    "    \n",
    "exp.stop(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca8a25-6e1b-4540-9d1e-932eb52d7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.get_status(ensemble, ml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b42065-6356-4a5a-b742-daca17b8bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.summary(format=\"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260d2f7-f986-4651-821a-aa5d0b25968d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartsim",
   "language": "python",
   "name": "smartsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
